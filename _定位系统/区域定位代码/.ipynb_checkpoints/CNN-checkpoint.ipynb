{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Phase2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2fae68c28243>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIPYNB_IMPORTER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mPhase2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mDivisionForP2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Phase2'"
     ]
    }
   ],
   "source": [
    "import IPYNB_IMPORTER\n",
    "import Phase2\n",
    "from DivisionForP2 import Dataset\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation,Dropout,Conv2D,MaxPooling2D, Flatten\n",
    "from keras.optimizers import RMSprop,Nadam,Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]} \n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('accuracy'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_accuracy'))\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        \n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('accuracy'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_accuracy'))\n",
    "\n",
    "    def loss_plot(self, loss_type,modelname):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "    \n",
    "        # acc\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.savefig(modelname+\"_acc.png\")\n",
    "        plt.show()\n",
    "\n",
    "        # loss\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.savefig(modelname+\"_los.png\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样本数据处理\n",
    "# 输入：训练集样本数据或者测试集样本数据\n",
    "# 输出：匹配CNN模型的输入数据\n",
    "def X_data_processing(X):\n",
    "    X[X==0] = -120\n",
    "    normalize = MinMaxScaler()\n",
    "    normalize.fit(X)\n",
    "    X = normalize.transform(X)\n",
    "    zero_test = np.zeros((len(X),15),dtype=float) #生成15个全零，用于扩充训练数据（469-》484=22*22）\n",
    "    X_process = np.concatenate((X,zero_test),axis=1)    # 延长每个向量到22*22\n",
    "    X_process = X_process.reshape(len(X_process),22,22)\n",
    "    X_process = np.expand_dims(X_process,axis=3)\n",
    "    return X_process\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 区域号数据处理\n",
    "# 输入：区域号\n",
    "# 输出：one-hot编码之后的区域号\n",
    "def area_data_processing(area):\n",
    "    area = area -1\n",
    "    area = to_categorical(area,32) \n",
    "    return area\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试写一个用于数据增强的数据生成器，这个生成器将放在fit_generator里面使用，直接生成增强数据\n",
    "## 它应该接收原本用于训练的数据集作为输入\n",
    "# X：原始样本，样本数不定，样本长度为469.未经过预处理。\n",
    "# y：原始标签，样本数随X，长度为3，（区域，x，y），仅使用区域，需进一步处理和提取。\n",
    "## 它应该输出一个用于训练的元组（gen_x,gen_y）\n",
    "######gen_y :按照训练集生成的虚拟样本的标签，要根据该标签生成样本，它应该是经过预处理之后的标签，而不是长度为3的原始样子。\n",
    "# gen_y的生成法：等概率地随机抽取区域。\n",
    "# gen_y的预处理法：区域标记序号自然数将转化为长度为32的one-hot向量。\n",
    "######gen_x：按照训练集生成的虚拟样本，它们应该依据样本特性随机产生，产生后需要经过预处理，以便直接投入模型训练。\n",
    "# gen_x的生成法：首先确定该次随机样本的区域，根据区域特点确定每个AP的随机规则*，依次随机AP的读数。\n",
    "# gen_x的预处理法：对0读数赋予-120的rssi值，再进行最大最小标准化。\n",
    "#####################\n",
    "# *随机规则：1.认为每个区域具有相同的属性。\n",
    "#            2.该AP在该区域是否在实际数据集中取到过值？\n",
    "#                Y: 计算取到值和未取到（0）之间的比例，从而计算未取到值（0）的概率p，进入下一行；\n",
    "#                   该AP的随机读数将有p的概率为0。若随机结果为“是0”，退出，结果为0；若随机结果是“不为0”，进入下一行；\n",
    "#                   计算该AP历史实际读数的最大值max、最小值min，该AP的随机读数r将在区间[min,max]随机产生，进入下一行；\n",
    "#                   添加随机波动，r = r ± 5，退出。\n",
    "#                N: 为0，退出。\n",
    "\n",
    "def datagen(X,y,normalizer,batch_size=32): #X是训练数据集，y是训练标签集\n",
    "    # 第一步：计算各个区域的每AP的属性。\n",
    "    # 属性：为0的概率p。\n",
    "    #       取值历史最小值min。\n",
    "    #       取值历史最大值max。\n",
    "    x_div_by_area,y_div_by_area = Phase2.DivByArea(X,y)#按区域进行划分\n",
    "    attr = {}\n",
    "    for area,samples in x_div_by_area.items():          #对于每一个区域\n",
    "        total = len(samples)    # 该区域样本总个数\n",
    "        zero = sum(samples == 0)  # 该区域每一列0的个数\n",
    "        ratio = zero/total       # 该区域样本每个ap取0的可能性\n",
    "        min_ = np.amin(samples,axis=0)   # 该区域可以取到的最小值\n",
    "        samples[samples==0] = -120\n",
    "        max_ = np.amax(samples,axis=0)   # 该区域可以取到的最大值\n",
    "        # 已计算出 ratio min_ max_\n",
    "        attr[area] = (ratio,min_,max_)\n",
    "    while True:  \n",
    "        vXs = [] #随机变换之后的样本集\n",
    "        vys = [] #随机变换之后的标签集   \n",
    "        for i in range(batch_size):\n",
    "            area = random.randint(1,32)     # 每一次随机选择样本所处区域\n",
    "            vX = []                # 预设一个虚拟X\n",
    "            #取出需要的区域参数\n",
    "            ratio = attr[area][0] #为0的概率\n",
    "            min_ = attr[area][1]  #最小的可能取值\n",
    "            max_ = attr[area][2] #最大的可能取值\n",
    "            for apnum in range(len(X[0])):\n",
    "                if random.random() < ratio[apnum]: #随机结果的概率小于该列取零的概率，即随机结果认为该AP取值应该为0 \n",
    "                    ##random.random() 返回随机生成的一个在[0,1]范围的实数\n",
    "                    vX.append(0)\n",
    "                else:         # 随机结果认为该ap取值不应为0\n",
    "                    temp = random.randint(min_[apnum],max_[apnum])#就在该列取值的最大值和最小值之间随机生成一个数\n",
    "                    error = random.randint(-5,5) #随机产生一个（-5，5）之间的误差\n",
    "                    temp = temp+error #随机结果认为该AP应该的取值\n",
    "                    vX.append(temp)\n",
    "            vXs.append(vX)\n",
    "            vys.append(area)\n",
    "            \n",
    "        ###################### 下面是预处理流程\n",
    "        vXs = np.array(vXs)   # 转为numpy数组\n",
    "        vys = np.array(vys) \n",
    "        vXs_exp = X_data_processing(vXs)#处理vXs数据\n",
    "        vys = area_data_processing(vys)\n",
    "        yield (vXs_exp,vys)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test,X_test = Dataset(\"Test_1.csv\")\n",
    "y_train,X_train = Dataset(\"Train_1.csv\")\n",
    "normalize = MinMaxScaler()\n",
    "X_test_exp = X_data_processing(X_test)#处理样本数据\n",
    "area_test = y_test[:,0]\n",
    "area_test = area_data_processing(area_test)\n",
    "area_true = np.argmax(area_test,axis=1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = Sequential([\n",
    "    Conv2D(32,(3,3),activation='relu',input_shape=(22,22,1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)), #最大池化层\n",
    "    Conv2D(32,(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),  \n",
    "    Dense(64,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "# 定义优化器\n",
    "rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "mymodel.compile(optimizer=rmsprop,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']) # metrics赋值为'accuracy'，会在训练过程中输出正确率\n",
    "\n",
    "history = LossHistory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.summary()\n",
    "plot_model(mymodel,to_file=\"./Picture/model_2.png\",show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Training ------------')\n",
    "mymodel.fit_generator(datagen(X_train,y_train,normalize), steps_per_epoch = 100, epochs = 50, validation_data=(X_test_exp,area_test), callbacks=[history])\n",
    "print('\\nTesting ------------')\n",
    "# 评价训练出的网络\n",
    "loss, accuracy = mymodel.evaluate(X_test_exp, area_test)\n",
    "\n",
    "print('test loss: ', loss)\n",
    "print('test accuracy: ', accuracy)\n",
    "history.loss_plot('epoch',\"./Phase_1/model_2\")\n",
    "\n",
    "mymodel.save(\"./Phase_1/model_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mymodel.predict_classes(X_test_exp))#返回类别的索引标签"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
